\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Assignment 1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Part 1: Binary classification and the perceptron}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Reading data}{1}}
\newlabel{sec:readdata}{{1.1.1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plot of the input vectors with the target value visualized by colour.}}{1}}
\newlabel{fig:inputVectors}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plot of the transformed input vectors with the target value visualized by colour.}}{2}}
\newlabel{fig:transformedInputVectors}{{2}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Perceptron training algorithm}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plot of the decision boundary in the original data space found by the perceptron (green curve) together with labelled data points.}}{2}}
\newlabel{fig:perceptron}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Perceptron decision boundary in the original data space at iterations \#1, \#3 and \#6 of online learning.}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}}
\newlabel{fig:origOL}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Perceptron decision boundary in the feature space of basis functions at iterations \#1, \#3 and \#6 of online learning.}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}}
\newlabel{fig:transOL}{{5}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Perceptron decision boundary in the original data space at iterations \#1, \#342 and \#685 of batch learning.}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}}
\newlabel{fig:origBA}{{6}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Perceptron decision boundary in the feature space of basis functions at iterations \#1, \#342 and \#685 of batch learning.}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}}
\newlabel{fig:transBA}{{7}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Part 2: Linear basis function models for regression}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Experimental setup}{3}}
\citation{Bishop}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Weight vector derived for the online LMS-rule and closed form}}{4}}
\newlabel{table:wLMS}{{1}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Optimization: \emph  {LMS}-learning rule vs. closed form}{4}}
\newlabel{fig:122_LMS_online}{{8(a)}{4}}
\newlabel{sub@fig:122_LMS_online}{{(a)}{4}}
\newlabel{fig:122_LMS_closed}{{8(b)}{4}}
\newlabel{sub@fig:122_LMS_closed}{{(b)}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Optimization using the LMS-rule with online learning (a) and in closed form (b)}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{4}}
\newlabel{fig:122_LMS}{{8}{4}}
\citation{hastie}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Plot of the number of iterations for gammas between 0.0001 and 0.005.}}{5}}
\newlabel{fig:bestGamma}{{9}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Model-complexity and model-selection}{5}}
\bibdata{lit}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  The expected value of the mean squared error, the bias and the variance against $d$ for a fixed $x'=2$ evaluated for 2000 trials, where $d$ denotes the dimension of the basis functions model}}{6}}
\newlabel{fig:NonReg}{{10}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Estimated MSE, Variance and Bias of non-regularized Model}}{6}}
\newlabel{table:EvalD}{{2}{6}}
\bibcite{Bishop}{1}
\bibcite{hastie}{2}
\bibstyle{unsrt}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The expected value of the mean squared error, the bias and the variance against the index of $\lambda $ for a fixed $x'=2$ evaluated for 2000 trials, where $\lambda $ denotes the weight decay parameter and is considered $\qopname  \relax o{exp}( \text  {index} - 4)$}}{7}}
\newlabel{fig:Reg}{{11}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Estimated MSE, Variance and Bias of regularized Model}}{7}}
\newlabel{table:EvalD}{{3}{7}}
